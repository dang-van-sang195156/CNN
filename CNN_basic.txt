Tính chất Invariance trong mạng CNN có ý nghĩa và tác dụng gì
Translation Invariance: Mạng CNN có khả năng nhận biết các đặc trưng không bị thay đổi bởi sự dịch chuyển của đối tượng trong hình ảnh. Nó có thể nhận diện một đối tượng ở bất kỳ vị trí nào trên hình ảnh.

Rotation Invariance: Tính năng này cho phép mạng CNN nhận biết các đối tượng dù chúng bị xoay hoặc quay trái phải.

Scale Invariance: Mạng CNN cũng có thể nhận diện đối tượng dù chúng có tỷ lệ thay đổi kích thước.

Viewpoint Invariance: Mạng CNN có thể nhận biết các đặc trưng của đối tượng dù chúng thay đổi góc nhìn hoặc quan điểm.


locality principle trong CNN có ý nghĩa và tác dụng gì

tập trung vào việc xử lý các đặc trưng cục bộ hoặc các khu vực nhỏ của hình ảnh trước khi kết hợp chúng để nhận biết đối tượng hoặc thông tin toàn cục hơn

Constraining the MLP trong CNN có ý nghĩa và tác dụng gì

Giới hạn trọng số (Weight Constraints): Ràng buộc trọng số có thể là một cách để kiểm soát độ lớn của các trọng số trong các lớp FC. Thông qua ràng buộc, ta có thể giảm khả năng overfitting (tương tự như L2 regularization) và làm cho mô hình tổng quát hóa tốt hơn. Một ví dụ phổ biến của ràng buộc trọng số là Max Norm Constraint, nơi các trọng số được giới hạn sao cho chúng không vượt quá một giá trị ngưỡng cố định.

Ràng buộc đầu ra (Output Constraints): Ràng buộc đầu ra có thể được sử dụng để giới hạn phạm vi của đầu ra của lớp MLP. Điều này có thể hữu ích trong các tình huống khi chúng ta biết rằng đầu ra cần nằm trong một phạm vi giá trị cụ thể hoặc theo một quy luật nào đó.

Thay đổi cấu trúc (Structural Constraints): Một loại ràng buộc khác là việc áp dụng cấu trúc cố định cho lớp MLP. Ví dụ, ràng buộc một số lớp chỉ chấp nhận các biểu đồ kết nối cụ thể giữa các đơn vị. Điều này có thể giúp mạng học cách biểu diễn các mẫu dữ liệu theo một cách cố định, giúp tạo sự ổn định và hiệu quả.

Hạn chế số lượng tham số (Parameter Constraints): Ràng buộc có thể giúp kiểm soát và giảm số lượng tham số trong mô hình. Điều này có thể giúp tăng khả năng tổng quát hóa, giảm nguy cơ overfitting và làm cho mô hình dễ quản lý hơn.
